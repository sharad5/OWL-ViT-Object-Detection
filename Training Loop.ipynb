{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.ops import box_convert\n",
    "import yaml\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import get_dataloaders\n",
    "from src.losses import ContrastiveDetectionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataset' from '/scratch/sd5251/cap/OWL4PACO/OWL-ViT-Object-Detection/src/dataset.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.dataset\n",
    "importlib.reload(src.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.utils' from '/scratch/sd5251/cap/OWL4PACO/OWL-ViT-Object-Detection/src/utils.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.utils\n",
    "importlib.reload(src.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils\n",
    "importlib.reload(src.losses)\n",
    "from src.losses import ContrastiveDetectionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\") # Image Processor + Text Tokenizer\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = get_dataloaders(4, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_config():\n",
    "    with open(\"config.yaml\", \"r\") as stream:\n",
    "        data = yaml.safe_load(stream)\n",
    "        return data[\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cfg = get_training_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveDetectionLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=float(training_cfg[\"learning_rate\"]),\n",
    "                weight_decay=training_cfg[\"weight_decay\"],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = training_cfg[\"n_epochs\"]\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 473.836: 100%|██████████| 100/100 [00:45<00:00,  2.58it/s]"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(training_cfg[\"n_epochs\"]):\n",
    "    for i, (inputs, target_labels, boxes, metadata) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs['input_ids'] = inputs['input_ids'].view(-1,16)\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].view(-1,16)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        \n",
    "        logits = outputs[\"logits\"]\n",
    "        pred_boxes = outputs[\"pred_boxes\"]\n",
    "        \n",
    "        batch_size = boxes.shape[0]\n",
    "        \n",
    "        target_labels = target_labels.to(device)\n",
    "        boxes = boxes.to(device)\n",
    "        \n",
    "        loss = criterion(logits, pred_boxes, boxes, target_labels, metadata)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes[:, :, 2:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = nn.functional.one_hot(torch.zeros(1).to(torch.int64), num_classes=num_queries).to(device)\n",
    "target_labels = target_labels.repeat(batch_size,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20231116_0021'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%Y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 512.766:   0%|                      | 12/45840 [00:18<13:16:56,  1.04s/it]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/sd5251/cap/OWL4PACO/OWL-ViT-Object-Detection/main.py\", line 58, in <module>\n",
      "    for i, (inputs, target_labels, boxes, metadata) in enumerate(train_dataloader):\n",
      "  File \"/ext3/miniconda3/envs/owl_boto/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/ext3/miniconda3/envs/owl_boto/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1328, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/ext3/miniconda3/envs/owl_boto/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1294, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/ext3/miniconda3/envs/owl_boto/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1132, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/ext3/miniconda3/envs/owl_boto/lib/python3.10/multiprocessing/queues.py\", line 113, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/ext3/miniconda3/envs/owl_boto/lib/python3.10/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/ext3/miniconda3/envs/owl_boto/lib/python3.10/multiprocessing/connection.py\", line 424, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/ext3/miniconda3/envs/owl_boto/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/ext3/miniconda3/envs/owl_boto/lib/python3.10/selectors.py\", line 416, in select\n"
     ]
    }
   ],
   "source": [
    "! python3 main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_boxes = torch.randn(4, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos_queries = torch.tensor([9,10,10,10,9,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = target_boxes.expand(4,10,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (10) must match the existing size (9) at non-singleton dimension 0.  Target sizes: [10, 4].  Tensor sizes: [9, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m a[\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m9\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (10) must match the existing size (9) at non-singleton dimension 0.  Target sizes: [10, 4].  Tensor sizes: [9, 4]"
     ]
    }
   ],
   "source": [
    "a[1] = a[1][:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(torch.tensor([527]), torch.tensor([0])), (torch.tensor([575]), torch.tensor([0])), (torch.tensor([575]), torch.tensor([0])), (torch.tensor([571]), torch.tensor([0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([527]), tensor([0])),\n",
       " (tensor([575]), tensor([0])),\n",
       " (tensor([575]), tensor([0])),\n",
       " (tensor([571]), tensor([0]))]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(a)[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [(torch.tensor([502, 503, 527, 550, 551, 571, 572, 573, 574, 575]), torch.tensor([9, 7, 4, 3, 2, 8, 6, 5, 1, 0])), (torch.tensor([262, 283, 284, 285, 306, 356, 357, 380, 405, 478]), torch.tensor([3, 2, 1, 0, 7, 4, 6, 9, 5, 8])), (torch.tensor([479, 503, 527, 550, 551, 571, 572, 573, 574, 575]), torch.tensor([8, 5, 2, 3, 1, 9, 7, 4, 6, 0])), (torch.tensor([455, 479, 503, 527, 550, 551, 572, 573, 574, 575]), torch.tensor([8, 7, 5, 4, 9, 2, 6, 3, 1, 0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([502, 503, 527, 550, 551, 571, 572, 573, 574, 575]),\n",
       "  tensor([9, 7, 4, 3, 2, 8, 6, 5, 1, 0])),\n",
       " (tensor([262, 283, 284, 285, 306, 356, 357, 380, 405, 478]),\n",
       "  tensor([3, 2, 1, 0, 7, 4, 6, 9, 5, 8])),\n",
       " (tensor([479, 503, 527, 550, 551, 571, 572, 573, 574, 575]),\n",
       "  tensor([8, 5, 2, 3, 1, 9, 7, 4, 6, 0])),\n",
       " (tensor([455, 479, 503, 527, 550, 551, 572, 573, 574, 575]),\n",
       "  tensor([8, 7, 5, 4, 9, 2, 6, 3, 1, 0]))]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.stack([torch.stack(t, dim=1) for t in b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[502,   9],\n",
       "         [503,   7],\n",
       "         [527,   4],\n",
       "         [550,   3],\n",
       "         [551,   2],\n",
       "         [571,   8],\n",
       "         [572,   6],\n",
       "         [573,   5],\n",
       "         [574,   1],\n",
       "         [575,   0]],\n",
       "\n",
       "        [[262,   3],\n",
       "         [283,   2],\n",
       "         [284,   1],\n",
       "         [285,   0],\n",
       "         [306,   7],\n",
       "         [356,   4],\n",
       "         [357,   6],\n",
       "         [380,   9],\n",
       "         [405,   5],\n",
       "         [478,   8]],\n",
       "\n",
       "        [[479,   8],\n",
       "         [503,   5],\n",
       "         [527,   2],\n",
       "         [550,   3],\n",
       "         [551,   1],\n",
       "         [571,   9],\n",
       "         [572,   7],\n",
       "         [573,   4],\n",
       "         [574,   6],\n",
       "         [575,   0]],\n",
       "\n",
       "        [[455,   8],\n",
       "         [479,   7],\n",
       "         [503,   5],\n",
       "         [527,   4],\n",
       "         [550,   9],\n",
       "         [551,   2],\n",
       "         [572,   6],\n",
       "         [573,   3],\n",
       "         [574,   1],\n",
       "         [575,   0]]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = torch.tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "\n",
    "        [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "\n",
    "        [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "\n",
    "        [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 40])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = torch.nn.functional.one_hot(torch.arange(10).to(torch.int64), num_classes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 40])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_labels.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(4).view(-1, 1).repeat(1, 10).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(10).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './checkpoints/20231116_0310_model.pt_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./checkpoints/20231116_0310_model.pt_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniconda3/envs/owl_boto/lib/python3.10/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/ext3/miniconda3/envs/owl_boto/lib/python3.10/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/ext3/miniconda3/envs/owl_boto/lib/python3.10/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checkpoints/20231116_0310_model.pt_model.pt'"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./checkpoints/20231116_0310_model.pt_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open(\"data/owlvit_train.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_file_name': '../paco_data/paco/paco_frames/2bd96ede-bc7b-45e9-bb34-e370c5dffc61_003995.jpeg',\n",
       " 'bbox': [1370.07, -1.58, 271.8, 815.41],\n",
       " 'pos_queries': ['A red towel',\n",
       "  'A striped, opaque, fabric towel',\n",
       "  'An opaque, fabric, red towel',\n",
       "  'A fabric towel',\n",
       "  'A striped, opaque, red towel',\n",
       "  'An opaque, fabric towel',\n",
       "  'A red, fabric towel',\n",
       "  'A striped towel',\n",
       "  'A fabric, red towel',\n",
       "  'A red towel',\n",
       "  'A striped towel',\n",
       "  'A striped, opaque towel',\n",
       "  'An opaque, red towel',\n",
       "  'A fabric towel',\n",
       "  'An opaque towel',\n",
       "  'An opaque, striped towel',\n",
       "  'A red, striped towel',\n",
       "  'A fabric, opaque, red towel',\n",
       "  'A red towel',\n",
       "  'A striped, red, opaque towel',\n",
       "  'A striped towel',\n",
       "  'An opaque, striped towel',\n",
       "  'A red, fabric towel',\n",
       "  'A red towel',\n",
       "  'A striped, red towel',\n",
       "  'A striped towel',\n",
       "  'An opaque, striped towel',\n",
       "  'An opaque towel',\n",
       "  'A fabric towel',\n",
       "  'A striped towel',\n",
       "  'An opaque, striped, fabric towel',\n",
       "  'A striped towel',\n",
       "  'A striped, opaque, red towel',\n",
       "  'A red towel',\n",
       "  'A fabric towel',\n",
       "  'A fabric towel',\n",
       "  'A red, fabric, striped towel',\n",
       "  'A fabric towel',\n",
       "  'An opaque, striped towel',\n",
       "  'A striped, red towel',\n",
       "  'A striped, opaque towel',\n",
       "  'A red, striped towel',\n",
       "  'A striped, red towel',\n",
       "  'A striped, red towel',\n",
       "  'A fabric, striped towel',\n",
       "  'A red, fabric towel',\n",
       "  'A fabric, striped, red towel',\n",
       "  'A red towel',\n",
       "  'An opaque, red towel',\n",
       "  'A striped, opaque, red towel'],\n",
       " 'neg_queries': ['A crochet towel',\n",
       "  'A velvet, checkered, transparent towel',\n",
       "  'A dark orange towel',\n",
       "  'A plain, paper towel',\n",
       "  'A ceramic towel with logo on it',\n",
       "  'A wood, plain towel',\n",
       "  'A transparent towel with text on it',\n",
       "  'A floral towel',\n",
       "  'A translucent, light orange towel',\n",
       "  'A transparent towel with text on it',\n",
       "  'A transparent towel',\n",
       "  'A velvet towel with text on it',\n",
       "  'A plastic, light green, translucent towel',\n",
       "  'A translucent towel with text on it',\n",
       "  'A crochet towel',\n",
       "  'A translucent, dark purple towel',\n",
       "  'A purple towel',\n",
       "  'A crochet, studded towel',\n",
       "  'A light red, ceramic towel',\n",
       "  'A black towel',\n",
       "  'A perforated, translucent, dark blue towel',\n",
       "  'A light yellow towel with logo on it',\n",
       "  'A transparent towel',\n",
       "  'A paper, studded, dark green towel',\n",
       "  'A translucent, checkered towel',\n",
       "  'A ceramic, checkered towel',\n",
       "  'A rattan towel',\n",
       "  'A dark grey, translucent towel',\n",
       "  'A transparent towel',\n",
       "  'A ceramic towel',\n",
       "  'A perforated, translucent towel',\n",
       "  'A translucent towel',\n",
       "  'A dark red, checkered, crochet towel',\n",
       "  'A light pink towel',\n",
       "  'A woven, light blue towel',\n",
       "  'A transparent towel',\n",
       "  'A crochet towel',\n",
       "  'A checkered, translucent, dark orange towel',\n",
       "  'A light green towel',\n",
       "  'A transparent, perforated towel',\n",
       "  'A transparent, glass towel',\n",
       "  'A towel with text on it',\n",
       "  'A metal, dark grey towel',\n",
       "  'A black, rattan, translucent towel',\n",
       "  'A glass, perforated towel',\n",
       "  'A translucent, light pink towel',\n",
       "  'A dotted towel',\n",
       "  'A dark pink towel',\n",
       "  'A crochet, pink, translucent towel',\n",
       "  'A rattan, grey towel',\n",
       "  'A plain towel',\n",
       "  'A yellow, transparent towel',\n",
       "  'A translucent towel',\n",
       "  'A velvet towel',\n",
       "  'A dark orange, metal towel',\n",
       "  'A pink, paper, dotted towel',\n",
       "  'A metal, translucent, dotted towel',\n",
       "  'A transparent towel',\n",
       "  'A metal, transparent towel',\n",
       "  'A translucent towel',\n",
       "  'A transparent, rattan, brown towel',\n",
       "  'A transparent towel',\n",
       "  'A wood, transparent towel',\n",
       "  'A woven towel',\n",
       "  'A dotted, leather towel',\n",
       "  'A translucent, plastic towel',\n",
       "  'A translucent towel',\n",
       "  'A wool, checkered, transparent towel',\n",
       "  'A velvet, light blue towel',\n",
       "  'A translucent, perforated towel',\n",
       "  'A translucent, studded, light orange towel',\n",
       "  'A studded, transparent towel',\n",
       "  'A light pink, studded towel',\n",
       "  'A plastic towel',\n",
       "  'A dotted towel',\n",
       "  'A transparent towel with text on it',\n",
       "  'A green, leather towel',\n",
       "  'A translucent, glass towel with logo on it',\n",
       "  'A woven towel',\n",
       "  'A translucent towel',\n",
       "  'A towel with text on it',\n",
       "  'A translucent, studded towel',\n",
       "  'A ceramic towel',\n",
       "  'A checkered, dark orange, transparent towel',\n",
       "  'A translucent towel',\n",
       "  'A transparent, light yellow towel',\n",
       "  'A dark green towel',\n",
       "  'A grey, transparent, wool towel',\n",
       "  'A translucent towel',\n",
       "  'A transparent towel',\n",
       "  'A checkered, translucent, ceramic towel',\n",
       "  'A perforated towel',\n",
       "  'A rattan towel',\n",
       "  'A dark orange, rattan towel',\n",
       "  'A leather, translucent towel',\n",
       "  'A woven towel',\n",
       "  'A leather towel',\n",
       "  'A transparent towel with text on it',\n",
       "  'A transparent, light red, perforated towel',\n",
       "  'A dark brown towel',\n",
       "  'A transparent, wool, perforated towel',\n",
       "  'A ceramic towel',\n",
       "  'A glass towel',\n",
       "  'A translucent, dotted, paper towel',\n",
       "  'A brown towel',\n",
       "  'A plain, ceramic towel',\n",
       "  'A dark purple, translucent towel',\n",
       "  'A metal towel',\n",
       "  'A transparent towel',\n",
       "  'A woven, translucent towel',\n",
       "  'A translucent towel',\n",
       "  'A woven, dark grey towel',\n",
       "  'A transparent, dark pink towel',\n",
       "  'A towel with text on it',\n",
       "  'A transparent, perforated, purple towel',\n",
       "  'A studded towel',\n",
       "  'A dark red, checkered towel',\n",
       "  'A white, metal towel',\n",
       "  'A transparent towel',\n",
       "  'A translucent, orange, ceramic towel',\n",
       "  'A transparent, white towel',\n",
       "  'A checkered, transparent towel',\n",
       "  'A crochet, transparent towel',\n",
       "  'A light red towel',\n",
       "  'A checkered towel',\n",
       "  'A transparent, wool, light pink towel',\n",
       "  'A light red towel',\n",
       "  'A translucent towel',\n",
       "  'A translucent, checkered towel',\n",
       "  'A metal towel',\n",
       "  'A translucent, rattan towel',\n",
       "  'A woven towel',\n",
       "  'A floral towel',\n",
       "  'A woven towel',\n",
       "  'A translucent towel',\n",
       "  'A plain towel',\n",
       "  'A translucent towel',\n",
       "  'A dark pink towel',\n",
       "  'A pink towel',\n",
       "  'A checkered, translucent towel',\n",
       "  'A transparent, plastic towel',\n",
       "  'A checkered, glass, translucent towel',\n",
       "  'A transparent towel',\n",
       "  'A stone, transparent towel with text on it',\n",
       "  'A light pink, transparent towel with logo on it',\n",
       "  'A paper towel with text on it',\n",
       "  'A transparent towel',\n",
       "  'A towel with logo on it',\n",
       "  'A light pink, glass towel with logo on it',\n",
       "  'A translucent, dotted towel',\n",
       "  'A leather, transparent towel',\n",
       "  'A transparent, pink towel',\n",
       "  'A metal, perforated towel',\n",
       "  'A wool towel',\n",
       "  'A translucent towel',\n",
       "  'A floral towel',\n",
       "  'A translucent, leather towel',\n",
       "  'A light grey, checkered towel',\n",
       "  'A translucent towel',\n",
       "  'A ceramic, transparent towel',\n",
       "  'A translucent, crochet towel',\n",
       "  'A transparent towel',\n",
       "  'A transparent towel with text on it',\n",
       "  'A dark blue towel',\n",
       "  'A translucent towel with text on it',\n",
       "  'A plastic towel',\n",
       "  'A dark blue towel',\n",
       "  'A dotted, light purple towel',\n",
       "  'A translucent, wool, plain towel',\n",
       "  'A wood, dark yellow towel',\n",
       "  'A floral towel',\n",
       "  'A translucent, wood towel with text on it',\n",
       "  'A dark pink, translucent towel',\n",
       "  'A translucent towel',\n",
       "  'A dark purple towel',\n",
       "  'A towel with logo on it',\n",
       "  'A plastic towel',\n",
       "  'A studded, pink, transparent towel',\n",
       "  'A checkered, dark pink, translucent towel',\n",
       "  'A translucent towel',\n",
       "  'A velvet, translucent, checkered towel',\n",
       "  'A glass, brown towel',\n",
       "  'A dotted towel',\n",
       "  'A glass, light green, translucent towel',\n",
       "  'A woven towel',\n",
       "  'A dark green, plain towel',\n",
       "  'A transparent towel',\n",
       "  'A floral, brown towel',\n",
       "  'A metal, checkered towel',\n",
       "  'A translucent, light blue towel',\n",
       "  'A paper, dark pink towel',\n",
       "  'A brown towel',\n",
       "  'A plain towel',\n",
       "  'A woven, dark yellow towel',\n",
       "  'A translucent, wood towel',\n",
       "  'A transparent towel',\n",
       "  'A plastic towel',\n",
       "  'A crochet, translucent, blue towel',\n",
       "  'A towel with text on it',\n",
       "  'A light red, perforated towel']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"annotations\"][910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"../paco_data/paco/paco_frames/2bd96ede-bc7b-45e9-bb34-e370c5dffc61_003995.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/hk3820/capstone/data/paco_frames/v1/paco_frames/2bd96ede-bc7b-45e9-bb34-e370c5dffc61_003995.jpeg'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\"/scratch/hk3820/capstone/data/paco_frames/v1/paco_frames\", os.path.basename(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OWL (Steve Bottos)",
   "language": "python",
   "name": "owl_boto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
