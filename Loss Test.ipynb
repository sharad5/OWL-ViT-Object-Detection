{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.ops import box_convert\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1553d46ab850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import get_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, test_dataloader = get_dataloaders(4)\n",
    "batch_1 = next(iter(train_dataloader))\n",
    "batch_1[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 675.3600,  326.0500, 1007.0600, 1052.7300],\n",
       "        [ 857.3300, 1010.7800,  185.2100,  285.7000],\n",
       "        [ 716.8300,  750.2000,  116.9500,   76.9800],\n",
       "        [1429.7600,  799.2400,  895.1000,  174.8000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': tensor([2560, 1920, 1920, 2560]),\n",
       " 'height': tensor([1920, 1440, 1080, 1920]),\n",
       " 'impath': ['/scratch/hk3820/capstone/data/paco_frames/v1/paco_frames/81cee65a-afe3-4dc2-a31e-3b67b062bf35_007471.jpeg',\n",
       "  '/scratch/hk3820/capstone/data/paco_frames/v1/paco_frames/69c9d98e-c125-4d24-b180-aea768ef900a_008159.jpeg',\n",
       "  '/scratch/hk3820/capstone/data/paco_frames/v1/paco_frames/3efc152d-ea0e-4372-b552-7d5e1cf07259_386360.jpeg',\n",
       "  '/scratch/hk3820/capstone/data/paco_frames/v1/paco_frames/a723d89c-78b7-4325-b18e-b8a4436a27ca_020117.jpeg']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1[0]['input_ids'] = batch_1[0]['input_ids'].view(-1,16)\n",
    "batch_1[0]['attention_mask'] = batch_1[0]['attention_mask'].view(-1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\") # Image Processor + Text Tokenizer\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PatchedOwlVit(OwlViTForObjectDetection):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#     def forward():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'pred_boxes', 'text_embeds', 'image_embeds', 'class_embeds', 'text_model_output', 'vision_model_output'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**batch_1[0].to(device))\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 576, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs[\"logits\"] # (B,N,C)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 576, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes = outputs[\"pred_boxes\"] # (B,N,4)\n",
    "pred_boxes.shape # Pred boxes in resized img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_sizes = torch.cat([batch_1[2]['height'].view(-1,1), batch_1[2]['width'].view(-1,1)], dim=1)\n",
    "# target_sizes = target_sizes.to(device)\n",
    "# results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_focal_loss(logits, focal_alpha = 0.5, focal_gamma = 2):\n",
    "    p = nn.functional.sigmoid(logits) # P(y=1)\n",
    "    pos_class_losses = -focal_alpha * torch.pow(1-p, focal_gamma) * torch.log(p) # Loss if y=1\n",
    "    neg_class_losses = -(1-focal_alpha) * torch.pow(p, focal_gamma) * torch.log(1-p) # Loss if y=0\n",
    "    return pos_class_losses, neg_class_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 576, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_class_losses, neg_class_losses =  pos_neg_focal_loss(logits, focal_alpha = 0.5, focal_gamma = 2)\n",
    "pos_class_losses.shape # (B,N,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = pos_class_losses.shape[0]\n",
    "num_queries = pos_class_losses.shape[-1]\n",
    "\n",
    "# Target Labels -- 1st query Pos, rest Neg\n",
    "target_labels = nn.functional.one_hot(torch.zeros(1).to(torch.int64), num_classes=num_queries).to(device)\n",
    "target_labels = target_labels.repeat(batch_size,1,1)\n",
    "target_labels.shape # (B,M,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_focal_loss(pos_class_losses, neg_class_losses, target_labels):\n",
    "    pos_class_loss = torch.einsum('bnc,bmc->bnm', pos_class_losses, target_labels.to(torch.float32)) # Sum of losses for pos queries\n",
    "    pos_class_loss = pos_class_loss/target_labels.sum(dim=-1).unsqueeze(-1) # Scale by num pos queries\n",
    "    neg_class_loss = torch.einsum('bnc,bmc->bnm', neg_class_losses, (1-target_labels.to(torch.float32))) # Sum of losses for neg queries\n",
    "    neg_class_loss = neg_class_loss/(1-target_labels).sum(dim=-1).unsqueeze(-1) # Scale by num neg queries\n",
    "    focal_loss = pos_class_loss + neg_class_loss\n",
    "    return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contrastive_focal_loss(pos_class_losses, neg_class_losses, target_labels):\n",
    "#     pos_class_weights = target_labels/target_labels.sum(dim=-1).unsqueeze(-1) # Indicator for +ve queries scaled by num_pos_queries\n",
    "#     neg_class_weights = (1-target_labels)/(1-target_labels).sum(dim=-1).unsqueeze(-1) # Indicator for -ve queries scaled by num_neg_queries\n",
    "#     pos_class_loss = torch.einsum('bnc,bmc->bnm', pos_class_losses, pos_class_weights) # Avg loss for pos queries\n",
    "#     neg_class_loss = torch.einsum('bnc,bmc->bnm', neg_class_losses, neg_class_weights) # Avg loss for neg queries\n",
    "#     focal_loss = pos_class_loss + neg_class_loss\n",
    "#     return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 576, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss = contrastive_focal_loss(pos_class_losses, neg_class_losses, target_labels)\n",
    "focal_loss.shape # (B,N,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = focal_loss.mean()\n",
    "# l.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBox loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes = box_convert(pred_boxes, \"cxcywh\", \"xyxy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_boxes = box_convert(batch_1[1], \"xywh\", \"xyxy\")\n",
    "target_boxes = target_boxes[:,None,:] # (B,4) -> (B,M=1,4)\n",
    "target_boxes = target_boxes.to(device)\n",
    "target_boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 576, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_dists = torch.abs(pred_boxes[:, :, None] - target_boxes[:, None, :])  # [B, N, M, 4]\n",
    "bbox_loss = torch.sum(coord_dists, axis=-1)  # [B, N, M]\n",
    "bbox_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = bbox_loss.mean()\n",
    "# l.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GIoU loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google-research/scenic/blob/main/scenic/model_lib/base_models/box_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 576, 4]), torch.Size([4, 1, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes.shape, target_boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(boxes1, boxes2, eps = 1e-6):\n",
    "    \"\"\"Computes IoU between two sets of boxes.\n",
    "\n",
    "    Boxes are in [x, y, x', y'] format [x, y] is top-left, [x', y'] is bottom right.\n",
    "\n",
    "    Args:\n",
    "        boxes1: Predicted bounding-boxes in shape [bs, n, 4].\n",
    "        boxes2: Target bounding-boxes in shape [bs, m, 4].\n",
    "        eps: Epsilon for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        Pairwise IoU cost matrix of shape [bs, n, m].\n",
    "    \"\"\"\n",
    "    # First, compute box areas. These will be used later for computing the union.\n",
    "    wh1 = boxes1[..., 2:] - boxes1[..., :2] # W & H of box1\n",
    "    area1 = wh1[..., 0] * wh1[..., 1]  # [bs, n]\n",
    "\n",
    "    wh2 = boxes2[..., 2:] - boxes2[..., :2]\n",
    "    area2 = wh2[..., 0] * wh2[..., 1]  # [bs, m]\n",
    "\n",
    "    # Compute pairwise top-left and bottom-right corners of the intersection of the boxes.\n",
    "    lt = torch.maximum(boxes1[..., :, None, :2], boxes2[..., None, :, :2])  # [bs, n, m, 2].\n",
    "    rb = torch.minimum(boxes1[..., :, None, 2:], boxes2[..., None, :, 2:])  # [bs, n, m, 2].\n",
    "\n",
    "    # intersection = area of the box defined by [lt, rb]\n",
    "    wh = (rb - lt).clip(0.0)  # [bs, n, m, 2]\n",
    "    intersection = wh[..., 0] * wh[..., 1]  # [bs, n, m]\n",
    "\n",
    "    # union = sum of areas - intersection\n",
    "    union = area1[..., :, None] + area2[..., None, :] - intersection\n",
    "\n",
    "    iou = intersection / (union + eps)\n",
    "    return iou, union  # pytype: disable=bad-return-type  # jax-ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_box_iou(boxes1, boxes2, eps = 1e-6):\n",
    "    \"\"\"Generalized IoU from https://giou.stanford.edu/.\n",
    "\n",
    "    The boxes should be in [x, y, x', y'] format specifying top-left and bottom-right corners.\n",
    "\n",
    "    Args:\n",
    "        boxes1: Predicted bounding-boxes in shape [..., N, 4].\n",
    "        boxes2: Target bounding-boxes in shape [..., M, 4].\n",
    "        eps: Epsilon for numerical stability.\n",
    "\n",
    "    Returns:\n",
    "        A [bs, n, m] pairwise matrix, of generalized ious.\n",
    "    \"\"\"\n",
    "    # Degenerate boxes gives inf / nan results, so do an early check.\n",
    "    assert (boxes1[:, :, 2:] >= boxes1[:, :, :2]).all()\n",
    "    assert (boxes2[:, :, 2:] >= boxes2[:, :, :2]).all()\n",
    "    \n",
    "    iou, union = box_iou(boxes1, boxes2, eps=eps)\n",
    "\n",
    "    # Generalized IoU has an extra term which takes into account the area of\n",
    "    # the box containing both of these boxes. The following code is very similar\n",
    "    # to that for computing intersection but the min and max are flipped.\n",
    "    lt = torch.minimum(boxes1[..., :, None, :2], boxes2[..., None, :, :2])  # [bs, n, m, 2]\n",
    "    rb = torch.maximum(boxes1[..., :, None, 2:], boxes2[..., None, :, 2:])  # [bs, n, m, 2]\n",
    "\n",
    "    # Now, compute the covering box's area.\n",
    "    wh = (rb - lt).clip(0.0)  # Either [bs, n, 2] or [bs, n, m, 2].\n",
    "    area = wh[..., 0] * wh[..., 1]  # Either [bs, n] or [bs, n, m].\n",
    "\n",
    "    # Finally, compute generalized IoU from IoU, union, and area.\n",
    "    # Somehow the PyTorch implementation does not use eps to avoid 1/0 cases.\n",
    "    return iou - (area - union) / (area + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 576, 1]), torch.Size([4, 576, 1]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou, union = box_iou(pred_boxes, target_boxes)\n",
    "iou.shape, union.shape # (B,N,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 576, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giou_loss = generalized_box_iou(pred_boxes, target_boxes)\n",
    "giou_loss.shape  # (B,N,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = giou_loss.mean()\n",
    "# l.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 576, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss_coef = 1.0/3\n",
    "bbox_loss_coef = 1.0/3\n",
    "giou_loss_coef = 1.0/3\n",
    "\n",
    "total_loss = focal_loss_coef * focal_loss + bbox_loss_coef * bbox_loss + giou_loss_coef + giou_loss\n",
    "total_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = total_loss.mean()\n",
    "# l.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hungarian Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DETR.matcher import HungarianMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import paco_to_owl_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_for_matcher = {\n",
    "    \"pred_logits\": outputs.logits.to(device),\n",
    "    \"pred_boxes\": outputs.pred_boxes.to(device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [{\"labels\": torch.tensor([0]).to(device), \"boxes\":box.to(device)} for box in paco_to_owl_box(batch_1[1][:, None, :], batch_1[2])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels': tensor([0], device='cuda:0'),\n",
       "  'boxes': tensor([[202.6080, 130.4200, 504.7260, 551.5121]], device='cuda:0')},\n",
       " {'labels': tensor([0], device='cuda:0'),\n",
       "  'boxes': tensor([[342.9320, 539.0827, 417.0161, 691.4561]], device='cuda:0')},\n",
       " {'labels': tensor([0], device='cuda:0'),\n",
       "  'boxes': tensor([[286.7320, 533.4755, 333.5121, 588.2169]], device='cuda:0')},\n",
       " {'labels': tensor([0], device='cuda:0'),\n",
       "  'boxes': tensor([[428.9280, 319.6960, 697.4580, 389.6160]], device='cuda:0')}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 34,   0],\n",
       "        [545,   0],\n",
       "        [ 45,   0],\n",
       "        [519,   0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(outputs_for_matcher, targets)\n",
    "matches = torch.tensor(matches)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 576, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1410.3622, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_matched_loss = total_loss[torch.arange(batch_size), matches[:,0], matches[:,1]]\n",
    "mean_loss = total_matched_loss.mean()\n",
    "mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owl-botu",
   "language": "python",
   "name": "owl-botu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
